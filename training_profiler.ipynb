{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "# %load_ext memory_profiler\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "from Runners.Ludo_AC_2p_v3 import run_game\n",
    "\n",
    "def print_full(returns, run_id):\n",
    "    now = datetime.now()\n",
    "    date_time = now.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    string = '%s_%s.txt' % (date_time, run_id)\n",
    "    file_path = \"results/\"\n",
    "    if not os.path.exists(file_path):\n",
    "        os.makedirs(file_path)\n",
    "    f = open(\"%s%s\" % (file_path, string), \"a\")\n",
    "\n",
    "    f.write(','.join(str(e) for e in returns[0]))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(','.join(str(e) for e in returns[1]))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(','.join(str(e) for e in returns[2]))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(','.join(str(e) for e in returns[3]))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(','.join(str(e) for e in returns[4]))\n",
    "    f.write(\"\\n\")\n",
    "    # f.write(','.join(str(e) for e in returns[5]))\n",
    "    # f.write(\"\\n\")\n",
    "    f.flush()\n",
    "    f.close()\n",
    "    return string\n",
    "\n",
    "\n",
    "def print_small(returns, run_id):\n",
    "    now = datetime.now()\n",
    "    date_time = now.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    string = '%s_%s.txt' % (date_time, run_id)\n",
    "    file_path = \"results/small/\"\n",
    "    if not os.path.exists(file_path):\n",
    "        os.makedirs(file_path)\n",
    "    f = open(\"results/small/%s.txt\" % string, \"a\")\n",
    "\n",
    "    f.write(','.join(str(e) for e in returns[0]))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(','.join(str(e) for e in returns[1]))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(','.join(str(e) for e in returns[2]))\n",
    "    f.write(\"\\n\")\n",
    "    f.flush()\n",
    "    f.close()\n",
    "\n",
    "    return string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-14 14:07:07.534249: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-10-14 14:07:07.534307: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: ds-rnd-t4-32c-02\n",
      "2022-10-14 14:07:07.534318: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: ds-rnd-t4-32c-02\n",
      "2022-10-14 14:07:07.534415: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 465.19.1\n",
      "2022-10-14 14:07:07.534449: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1\n",
      "2022-10-14 14:07:07.534456: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 465.19.1\n",
      "2022-10-14 14:07:07.534851: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "100%|##########| 5/5 [01:40<00:00, 20.19s/e]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN 0008 complete\n",
      "20221014_140848_0008.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 101.361 s\n",
      "File: /nfs_storage/fs-mnt6/barathmohanU/ludo_bot/LudoRL/Runners/Ludo_AC_2p_v3.py\n",
      "Function: run_game at line 14\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    14                                           def run_game(num_ep, model_output):\n",
      "    15         1        494.0    494.0      0.0      env = FullBoard()\n",
      "    16         1     412428.0 412428.0      0.4      agent0 = Agent(n_actions=env.action_size(), input_dim=env.state_size(), alpha=1e-8, max_val=env.max_val())\n",
      "    17                                           \n",
      "    18         1          2.0      2.0      0.0      agent0_reward = []\n",
      "    19         1          1.0      1.0      0.0      agent1_reward = []\n",
      "    20         1          1.0      1.0      0.0      agent2_reward = []\n",
      "    21         1          1.0      1.0      0.0      agent3_reward = []\n",
      "    22         1          1.0      1.0      0.0      episode_length = []\n",
      "    23                                           \n",
      "    24         1          2.0      2.0      0.0      output_dir_a = 'model_output/A2C/%s/actor/' % model_output\n",
      "    25         1          1.0      1.0      0.0      output_dir_c = 'model_output/A2C/%s/critic/' % model_output\n",
      "    26                                           \n",
      "    27         1        801.0    801.0      0.0      if not os.path.exists(output_dir_a):\n",
      "    28                                                   os.makedirs(output_dir_a)\n",
      "    29         1        206.0    206.0      0.0      if not os.path.exists(output_dir_c):\n",
      "    30                                                   os.makedirs(output_dir_c)\n",
      "    31                                           \n",
      "    32         6      10744.0   1790.7      0.0      for ep in tqdm(range(0, num_ep), ascii=True, unit=\"e\"):\n",
      "    33         5          9.0      1.8      0.0          step = 0\n",
      "    34         5       1574.0    314.8      0.0          s, _, game_over, player_turn = env.reset()\n",
      "    35         5          7.0      1.4      0.0          episode_reward = [0.0, 0.0, 0.0, 0.0]\n",
      "    36       618        785.0      1.3      0.0          while not game_over:\n",
      "    37                                           \n",
      "    38       613       3964.0      6.5      0.0              player_turn_temp = env.get_player_turn()\n",
      "    39       613      37542.0     61.2      0.0              env.roll_dice()[0]\n",
      "    40       613        970.0      1.6      0.0              player_turn = env.get_player_turn()\n",
      "    41                                           \n",
      "    42       613        683.0      1.1      0.0              if player_turn == player_turn_temp:\n",
      "    43                                           \n",
      "    44       612     138178.0    225.8      0.1                  action_list = env.get_next_states(player_turn)\n",
      "    45                                           \n",
      "    46       612        758.0      1.2      0.0                  if action_list:\n",
      "    47       609        633.0      1.0      0.0                      if player_turn > -1:\n",
      "    48                                                               # if player_turn % 2 == 0:\n",
      "    49       609       7602.0     12.5      0.0                          s_t = env.convert_state(player_turn)\n",
      "    50       609    7753466.0  12731.5      7.6                          action = agent0.act(s_t, action_list)\n",
      "    51                                                               else:\n",
      "    52                                                                   action = choose_rand(action_list)\n",
      "    53                                           \n",
      "    54       609      41265.0     67.8      0.0                      s_, reward, game_over, player_turn_temp = env.make_step(action)\n",
      "    55                                           \n",
      "    56       609        778.0      1.3      0.0                      if player_turn > -1:\n",
      "    57                                                               # if player_turn % 2 == 0:\n",
      "    58       609       8095.0     13.3      0.0                          s_t_ = env.convert_state(player_turn)\n",
      "    59       609   32711544.0  53713.5     32.3                          agent0.learn(s_t, action, reward[player_turn], s_t_, game_over)\n",
      "    60       609   60183140.0  98822.9     59.4                          gc.collect()\n",
      "    61                                           \n",
      "    62       609       5839.0      9.6      0.0                      episode_reward[player_turn] += reward[player_turn]\n",
      "    63                                           \n",
      "    64       609       1015.0      1.7      0.0                      step += 1\n",
      "    65                                           \n",
      "    66       613        635.0      1.0      0.0              if game_over:\n",
      "    67         5         17.0      3.4      0.0                  agent0_reward.append(episode_reward[0])\n",
      "    68         5          5.0      1.0      0.0                  agent1_reward.append(episode_reward[1])\n",
      "    69         5          6.0      1.2      0.0                  agent2_reward.append(episode_reward[2])\n",
      "    70         5          6.0      1.2      0.0                  agent3_reward.append(episode_reward[3])\n",
      "    71         5         13.0      2.6      0.0                  episode_length.append(step / 4)\n",
      "    72                                           \n",
      "    73         5          7.0      1.4      0.0          if ep > 1000:\n",
      "    74                                                       agent0.reduce_alpha()\n",
      "    75                                           \n",
      "    76         5          9.0      1.8      0.0          if ep % 100 == 0:\n",
      "    77                                                       # print(np.average(agent0_reward[-1000:]), np.average(agent1_reward[-1000:]))\n",
      "    78         1          8.0      8.0      0.0              agent0.save_model(output_dir_a + \"weights_\" + '{:04d}'.format(ep) + \".hdf5\", \n",
      "    79         1      38165.0  38165.0      0.0                                output_dir_c + \"weights_\" + '{:04d}'.format(ep) + \".hdf5\")\n",
      "    80                                           \n",
      "    81         1          2.0      2.0      0.0      return [agent0_reward, agent1_reward, agent2_reward, agent3_reward, episode_length]"
     ]
    }
   ],
   "source": [
    "episodes = 5\n",
    "run_no = 8\n",
    "run_id = s1 = f'{run_no:04d}'\n",
    "\n",
    "%lprun -f run_game returns = run_game(episodes, run_id)\n",
    "\n",
    "string = print_full(returns, run_id)\n",
    "# string = print_small(returns, run_id)\n",
    "\n",
    "print(\"RUN %s complete\" % run_id)\n",
    "print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('rl-env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a2c9548d151a78bce28fe03d6b80e2a870126e154014100318ab730d04413d79"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
